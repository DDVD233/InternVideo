# VLN-CE Downstream
This is an official implementation of the visual-language navigation task in [InternVideo](https://arxiv.org/abs/2212.03191).

## Running the code

We currently provide evaluation of our pretrained model.

### Conda environment preperation
1. Please follow https://github.com/jacobkrantz/VLN-CE to install Habitat Simulator and Habitat-lab. We use Python 3.6 in our experiments.
2. Follow https://github.com/openai/CLIP to install CLIP.

### Data/Model preperation
The pretrained models and data for running the code will be released soon.

### Running the code
Simply run `bash eval_**.sh` to start evaluating the agent.